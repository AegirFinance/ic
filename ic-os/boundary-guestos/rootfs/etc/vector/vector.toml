[sources.vector_metrics]
type = "internal_metrics"

[sinks.vector_exporter]
type = "prometheus_exporter"
inputs = ["vector_metrics"]
address = "0.0.0.0:9598"
default_namespace = "vector"

# nginx

[sources.nginx]
type = "journald"
include_units = ["nginx"]

# nginx access

[transforms.nginx_access]
type = "filter"
inputs = ["nginx"]
condition = ".SYSLOG_IDENTIFIER == \"access\""

[transforms.nginx_access_json]
type = "remap"
inputs = ["nginx_access"]
source = """
. = parse_json!(.message)

.@timestamp, err = to_float(.msec) * 1000
if err != null {
    .@timestamp = null
}

.@timestamp = to_int(.@timestamp)
if .@timestamp == 0 {
    .@timestamp = null
}

for_each([
    "ic_subnet_id",
    "ic_node_id",
    "ic_canister_id",
]) -> |_, k| {
    if get!(., [k]) == "" {
        . = set!(., [k], "N/A")
    }
}
"""

# nginx access (elasticsearch)

[sinks.elasticsearch_nginx_access]
type = "elasticsearch"
inputs = ["nginx_access_json"]
endpoint = "${ELASTICSEARCH_URL}"
mode = "bulk"
bulk.index = "boundary-node-nginx-access-%Y.%m.%d"
tls.verify_certificate = false

# nginx access (metrics)

[transforms.nginx_access_metrics]
type = "log_to_metric"
inputs = ["nginx_access_json"]

  [[transforms.nginx_access_metrics.metrics]]
  type = "counter"
  field = "status"
  name = "request_total"

    [transforms.nginx_access_metrics.metrics.tags]
    hostname = "{{ hostname }}"
    ic_node_id = "{{ ic_node_id }}"
    ic_subnet_id = "{{ ic_subnet_id }}"
    request_method = "{{ request_method }}"
    status = "{{ status }}"
    upstream_cache_status = "{{ upstream_cache_status }}"
    upstream_status = "{{ upstream_status }}"

  [[transforms.nginx_access_metrics.metrics]]
  type = "histogram"
  field = "request_time"
  name = "request_sec_duration"

    [transforms.nginx_access_metrics.metrics.tags]
    hostname = "{{ hostname }}"
    ic_node_id = "{{ ic_node_id }}"
    ic_subnet_id = "{{ ic_subnet_id }}"
    request_method = "{{ request_method }}"
    status = "{{ status }}"
    upstream_cache_status = "{{ upstream_cache_status }}"
    upstream_status = "{{ upstream_status }}"

# nginx error

[transforms.nginx_error]
type = "filter"
inputs = ["nginx"]
condition = ".SYSLOG_IDENTIFIER == \"error\""

[transforms.nginx_error_json]
type = "remap"
inputs = ["nginx_error"]
source = """
.@timestamp, err = to_int(.__REALTIME_TIMESTAMP)
if err != null {
    .@timestamp = null
}

.@timestamp, err = .@timestamp / 1000
if err != null {
    .timestamp = null
}
.@timestamp = to_int(.@timestamp)

. = {
    "@timestamp": .@timestamp,
    "host": .host,
    "message": .message
}
"""

# nginx error (elasticsearch)

[sinks.elasticsearch_nginx_error]
type = "elasticsearch"
inputs = ["nginx_error_json"]
endpoint = "${ELASTICSEARCH_URL}"
mode = "bulk"
bulk.index = "boundary-node-nginx-error-%Y.%m.%d"
tls.verify_certificate = false

# nginx error (metrics)

[transforms.nginx_error_metrics]
type = "log_to_metric"
inputs = ["nginx_error_json"]

  [[transforms.nginx_error_metrics.metrics]]
  type = "counter"
  field = "message"
  name = "error_total"

    [transforms.nginx_error_metrics.metrics.tags]
    hostname = "{{ host }}"

# nginx (prometheus)

[sinks.prometheus_exporter_nginx]
type = "prometheus_exporter"
inputs = ["nginx_access_metrics", "nginx_error_metrics"]
address = "${NGINX_PROMETHUS_ADDR}"
default_namespace = "nginx_access"

# prober

[sources.prober]
type = "file"
include = ["/var/log/prober/prober.log.*"]

[transforms.prober_json]
type = "remap"
inputs = ["prober"]
source = """
. = parse_json!(.message)

.@timestamp = to_timestamp!(.timestamp, unit: "milliseconds")
.@timestamp = to_unix_timestamp!(.@timestamp, unit: "milliseconds")
del(.timestamp)
"""

[sinks.elasticsearch_prober]
type = "elasticsearch"
inputs = ["prober_json"]
endpoint = "${ELASTICSEARCH_URL}"
mode = "bulk"
bulk.index = "boundary-node-prober-%Y.%m.%d"
tls.verify_certificate = false

# control-plane

[sources.control_plane]
type = "journald"
include_units = ["control-plane"]

[transforms.control_plane_json]
type = "remap"
inputs = ["control_plane"]
source = """
d = {}; ks = ["host"]

for_each(ks) -> |_, k| {
    v = get!(., [k])
    if v != null {
        d = set!(d, [k], v)
    }
}

. = parse_json!(.message)

.@timestamp = to_timestamp!(.timestamp, unit: "milliseconds")
.@timestamp = to_unix_timestamp!(.@timestamp, unit: "milliseconds")
del(.timestamp)

. = merge!(., d)
"""

[sinks.elasticsearch_control_plane]
type = "elasticsearch"
inputs = ["control_plane_json"]
endpoint = "${ELASTICSEARCH_URL}"
mode = "bulk"
bulk.index = "boundary-node-control-plane-%Y.%m.%d"
tls.verify_certificate = false
