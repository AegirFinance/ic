= IC_Replica_CriticalError
:icons: font
ifdef::env-github,env-browser[:outfilesuffix:.adoc]

== Triggered by

A `critical_error` counter was incremented by a replica. This `CounterVec`
(indexed by an `error` label) is used to record "not supposed to happen"
critical errors (within any component) that don't represent an immediate threat
to the health of the replica (and thus don't require panicking; essentially a
`panic!()` without the mess) but are definitely not expected to happen and the
component owners want to be notified immediately if and when they do.

Incrementing the counter is always supposed to be paired with logging an error
message beginning with the exact same label (to make it easy to find) providing
the context necessary for debugging.

Every single `critical_error` variant MUST be documented here, including impact,
possible causes and remediation, where necessary.

== Critical Errors

Critical error names are namespaced, similar to Prometheus metrics. The team
responsible for a given error counter can be determined from the error name
prefix:

 * `mr` and `state_manager`: Message Routing team, reachable at
https://dfinity.slack.com/archives/CKXPC1928[`#eng-messaging`] or via
`+@team-messaging+`on Slack.
 * `cycles_account_manager` and `scheduler`: Execution team, reachable at
 https://dfinity.slack.com/archives/CGZJ7G1J6[`#eng-execution`] or via
 `+@team-execution+`on Slack.
 * `consensus`: Consensus team, reachable at
 https://dfinity.slack.com/archives/CGZF2NKGT[`#eng-consensus`] or via
 `+@team-consensus` on Slack.

=== `cycles_account_manager_response_cycles_refund_error`

Cycles Account Manager did not perform a refund of leftover cycles from the
response (those cycles were reserved in the corresponding request sent
earlier), since the response was bigger than the max reserved amount.

An occurrence would indicate unexpectedly big response payload size.

Escalate to the Execution team.

=== `cycles_account_manager_missing_subnet_size_error`

Cycles Account Manager did not receive a subnet size before executing the current
round, which is required to scale cycles cost according to a subnet replication
factor.

An occurrence would indicate a broken network topology or an incorrect subnet ID.

Escalate to the Execution team.

=== `mr_induct_response_failed`

Inducting a canister `Response` has failed. There are two possible causes for
this, both unexpected:

 * Lack of resources: full input queue; or out of memory. We make explicit
   reservations for both (input queue slots; and response memory), so this
   would indicate a bug in the reservation logic.
 * Destination canister not found: a canister should not be deleted with open
   call contexts; and responses to migrated canisters should be rerouted.

Escalate to the Message Routing team.

=== `mr_receiver_subnet_mismatch`, `mr_sender_subnet_mismatch`

A canister message was not routed to the right subnet (according to the routing
table) while the destination canister is not being migrated
(`mr_receiver_subnet_mismatch`). Or a canister message was not received from the
right subnet while the source canister is not being migrated
(`mr_receiver_subnet_mismatch`). This likely means one of 3 things:

 * A bug in the routing logic.
 * A canister migration that was completed too early (while canister messages
   were still in flight).
 * A malicious sender subnet.

Escalate to the Message Routing team.

=== `mr_reject_signals_for_request`

A reject signal was received for a canister request. Reject signals are only
used to reroute responses during canister migrations. Misrouted requests result
in reject responses instead.

There are two possible causes for this error:

 * A bug leading to the two subnets disagreeing on stream indices (e.g. signal
   actually refers to a response, but is off-by-one).
 * A malicious remote subnet.

Escalate to the Message Routing team.

=== `mr_stream_builder_infinite_loop`

An infinite loop was detected while iterating over canister output queues as
part of routing output messages into streams. An occurrence of this error
would indicate a bug in the `PeekableOutputIterator` implementation.

Unless a stream is found to have stalled (which should not be the case, as at
least one message is routed every round) no immediate action is needed.

=== `mr_stream_builder_payload_too_large`

A canister response (either from another canister or from the system) with a
payload larger than `MAX_INTER_CANISTER_PAYLOAD_IN_BYTES` was encountered by
the routing logic and dropped. We reserve `MAX_INTER_CANISTER_PAYLOAD_IN_BYTES`
for every expected response and rely on this limit in order to avoid stalling
streams or bulding oversized blocks.

This indicates a bug in Execution or some system component that produces
canister responses (i.e. not enforcing `MAX_INTER_CANISTER_PAYLOAD_IN_BYTES`).

Escalate to the responsible team (response is logged). Fall back to Message
Routing team if unsure.

=== `mr_stream_builder_response_destination_not_found`

Message Routing is unable to route a canister response. Since we validate the
origin of incoming messages and we never unmap canister ranges in the routing
table, this would indicate a bug in MR code.

Escalate to the Message Routing team.

=== `sandboxed_execution_invalid_memory_size`

The memory size returned by the sandbox process is smaller than the actual Wasm/stable memory of a canister.
This indicates either a bug in the sandbox code or malicious code controlling the sandbox process.

Escalate to the Runtime team.

=== `scheduler_canister_invariant_broken`

At least one canister has an invalid state after the execution round.
It could indicate a bug in the code that breaks the canister state invariants.
Based on the replica logs, further investigations are needed to determine which canister has an invalid state and what invariant does not hold anymore.
Check if a recent replica upgrade has happened that could have introduced a code bug.

Escalate to the Execution team.

=== `scheduler_subnet_memory_usage_invariant_broken`

At least one subnet where the total amount of storage used by all canisters exceeds the maximum amount of logical storage available.
It could indicate a bug in the code, resulting in canisters exceeding the subnet storage limit allocated.
Further checks are needed to verify if the invariant is broken on different subnets. This could be caused by a recent upgrade that could have introduced a bug.

Escalate to the Execution team.

=== `state_manager_manifest_reused_chunk_hash_error_count`

State Manager uses page deltas to keep track of which Replicated State chunks
have not changed between checkpoints and reuses the corresponding chunk hashes
instead of computing them from scratch every CUP interval. But it also
probabilistically validates these hashes. This error would indicate a mismatch
between the reused and recomputed chunk hash.

An occurrence would indicate either a random bit flip or, if this happens on
multiple replicas, a bug that must be investigated immediately.

=== `state_sync_corrupted_chunks`

This is very similar to `state_manager_manifest_reused_chunk_hash_error_count`
above, except the hash mismatch is detected during state sync, for a chunk
that the State Sync implementation had assumed was already present locally.

An occurrence would indicate either a random bit flip or, if this happens on
multiple replicas, a bug that must be investigated immediately.


=== `consensus_subnet_record_issue`

The consensus component reads a number of values from the `SubnetRecord` for
that particular subnet. It is possible, that these values are set up in a way
that don't make sense to consensus, or that are obviously wrong, and would
stall the subnet.

In these cases, consenus internally replaces these values with sensible defaults
and raises this critical error.

The logs will contain information about which values are not set correctly.
This is usually not an immediate problem.
Use `ic-admin get-topology` to verify the invalid setting of the subnet record.
Then, either submit a proposal to fix the value or contact the consensus team.

=== `consensus_validation_not_passed`

The consensus component constructs a payload as part of building a block.
To defend against corrupt block makers, payloads are being validated on the
receiving side, before being processed.
In theory, a non-corrupt replica will always be able to build a valid response.
In practice however, a bug either in the payload builder code or the payload
validator code can lead to a situation where all non-corrupt replicas build
invalid blocks too. If this happens the subnet would stall.

To prevent this from happening, payload builders immediately run their own
validation code against the freshly build payload.
If the validation fails, the payload is replaced with an empty payload to not
stall the subnet and this critical error is risen.

This error indicates a critical bug in payload builder or validation code.
Contact the consensus team as soon as possible about this occurence.

=== `consensus_payload_too_large`

This error is similar to `consensus_validation_not_passed`.
Aside from validating the payload's content, the payload builder and
validator also have to make sure that the block is not oversized.

Similarly, a bug in either payload builder or validator can lead to a
situation where the builder builds a payload that is considered oversized
by the validator and is rejected.

This error indicates a critical bug in payload builder or validation code.
Contact the consensus team as soon as possible about this occurence.